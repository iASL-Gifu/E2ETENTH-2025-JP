import torch
import torch.nn as nn
import torch.nn.functional as F
import os
import hydra
from omegaconf import DictConfig, OmegaConf
from datetime import datetime


# å‡çµæ©Ÿèƒ½ä»˜ãActorã‚¯ãƒ©ã‚¹ï¼ˆãƒ­ãƒ¼ã‚«ãƒ«å®šç¾©ã¾ãŸã¯importå¯èƒ½ï¼‰
class Actor(nn.Module):
    def __init__(self, lidar_dim, action_dim=2, hidden_dim=256, freeze_feature_layers=False):
        super(Actor, self).__init__()
        self.action_dim = action_dim
        
        # LiDARãƒ‡ãƒ¼ã‚¿ã‚’ç›´æ¥å‡¦ç†ã™ã‚‹å…¨çµåˆå±¤
        self.fc1 = nn.Linear(lidar_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, hidden_dim)
        self.fc3 = nn.Linear(hidden_dim, hidden_dim)
        
        # å¹³å‡ã¨å¯¾æ•°æ¨™æº–åå·®ã®å‡ºåŠ›ãƒ˜ãƒƒãƒ‰
        self.mean_head = nn.Linear(hidden_dim, action_dim)
        self.log_std_head = nn.Linear(hidden_dim, action_dim)
        
        # âœ… ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨ã—ã¦ç™»éŒ²ï¼ˆregister_bufferã‚’ä½¿ç”¨ï¼‰
        self.register_buffer('action_scale', torch.FloatTensor([1.0, 1.0]))
        self.register_buffer('action_bias', torch.FloatTensor([0.0, 0.0]))
        
        # ğŸ”¥ ç‰¹å¾´æŠ½å‡ºå±¤ã‚’å‡çµ
        if freeze_feature_layers:
            self.freeze_feature_layers()
        
    def freeze_feature_layers(self):
        """fc1, fc2, fc3ã®3ã¤ã®å…¨çµåˆå±¤ã‚’å‡çµ"""
        print("ç‰¹å¾´æŠ½å‡ºå±¤(fc1, fc2, fc3)ã‚’å‡çµã—ã¦ã„ã¾ã™...")
        
        # fc1, fc2, fc3ã®å‹¾é…è¨ˆç®—ã‚’ç„¡åŠ¹åŒ–
        for param in self.fc1.parameters():
            param.requires_grad = False
        for param in self.fc2.parameters():
            param.requires_grad = False
        for param in self.fc3.parameters():
            param.requires_grad = True
            
        print("å‡çµå®Œäº†: fc1, fc2, fc3")
    
    def unfreeze_feature_layers(self):
        """fc1, fc2, fc3ã®3ã¤ã®å…¨çµåˆå±¤ã®å‡çµã‚’è§£é™¤"""
        print("ç‰¹å¾´æŠ½å‡ºå±¤(fc1, fc2, fc3)ã®å‡çµã‚’è§£é™¤ã—ã¦ã„ã¾ã™...")
        
        # fc1, fc2, fc3ã®å‹¾é…è¨ˆç®—ã‚’æœ‰åŠ¹åŒ–
        for param in self.fc1.parameters():
            param.requires_grad = True
        for param in self.fc2.parameters():
            param.requires_grad = True
        for param in self.fc3.parameters():
            param.requires_grad = True
            
        print("å‡çµè§£é™¤å®Œäº†: fc1, fc2, fc3")
    
    def check_frozen_status(self):
        """å‡çµçŠ¶æ…‹ã‚’ãƒã‚§ãƒƒã‚¯"""
        fc1_frozen = not any(p.requires_grad for p in self.fc1.parameters())
        fc2_frozen = not any(p.requires_grad for p in self.fc2.parameters())
        fc3_frozen = not any(p.requires_grad for p in self.fc3.parameters())
        mean_head_frozen = not any(p.requires_grad for p in self.mean_head.parameters())
        log_std_head_frozen = not any(p.requires_grad for p in self.log_std_head.parameters())
        
        print(f"å‡çµçŠ¶æ…‹:")
        print(f"  fc1: {'å‡çµ' if fc1_frozen else 'å­¦ç¿’å¯èƒ½'}")
        print(f"  fc2: {'å‡çµ' if fc2_frozen else 'å­¦ç¿’å¯èƒ½'}")
        print(f"  fc3: {'å‡çµ' if fc3_frozen else 'å­¦ç¿’å¯èƒ½'}")
        print(f"  mean_head: {'å‡çµ' if mean_head_frozen else 'å­¦ç¿’å¯èƒ½'}")
        print(f"  log_std_head: {'å‡çµ' if log_std_head_frozen else 'å­¦ç¿’å¯èƒ½'}")
        
        return {
            'fc1': fc1_frozen,
            'fc2': fc2_frozen, 
            'fc3': fc3_frozen,
            'mean_head': mean_head_frozen,
            'log_std_head': log_std_head_frozen
        }
        
    def forward(self, lidar_data):
        # LiDARãƒ‡ãƒ¼ã‚¿ã‚’ç›´æ¥å‡¦ç†
        x = F.relu(self.fc1(lidar_data))
        x = F.relu(self.fc2(x))
        x = F.relu(self.fc3(x))
        
        mean = self.mean_head(x)
        log_std = self.log_std_head(x)
        log_std = torch.clamp(log_std, min=-20, max=2)  # å®‰å®šæ€§ã®ãŸã‚ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°
        
        return mean, log_std
    
    def sample(self, lidar_data):
        """ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°"""
        mean, log_std = self.forward(lidar_data)
        std = log_std.exp()
        normal = torch.distributions.Normal(mean, std)
        
        # å†ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒ–ãƒˆãƒªãƒƒã‚¯
        x_t = normal.rsample()
        y_t = torch.tanh(x_t)
        action = y_t * self.action_scale + self.action_bias
        
        # ãƒ­ã‚°ç¢ºç‡ã‚’è¨ˆç®—
        log_prob = normal.log_prob(x_t)
        log_prob -= torch.log(self.action_scale * (1 - y_t.pow(2)) + 1e-6)
        log_prob = log_prob.sum(1, keepdim=True)
        
        # æ±ºå®šè«–çš„ãªå¹³å‡ã‚¢ã‚¯ã‚·ãƒ§ãƒ³
        mean = torch.tanh(mean) * self.action_scale + self.action_bias
        
        return action, log_prob, mean


# Importæ–‡ï¼ˆå…ƒã®ã‚³ãƒ¼ãƒ‰ã‹ã‚‰å¿…è¦ã«å¿œã˜ã¦è¿½åŠ ï¼‰
# from src.data.dataset.dataset import HybridLoader 
# from src.data.dataset.transform import SeqToSeqTransform, StreamAugmentor
# from src.models.layers.state_manager import RnnStateManager 


def load_sac_checkpoint(path, device):
    """SACãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆå…¨ä½“ã‚’ãƒ­ãƒ¼ãƒ‰ã™ã‚‹"""
    if not os.path.exists(path):
        raise FileNotFoundError(f"Model file not found: {path}")
    
    checkpoint = torch.load(path, map_location=device)
    print(f"Loaded SAC checkpoint with keys: {list(checkpoint.keys())}")
    return checkpoint


def create_actor_from_checkpoint(checkpoint, downsample_num, action_dim, hidden_dim, device, freeze_features=False):
    """ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‹ã‚‰Actorãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆï¼ˆå‡çµã‚ªãƒ—ã‚·ãƒ§ãƒ³ä»˜ãï¼‰"""
    # ğŸ”¥ å‡çµæ©Ÿèƒ½ä»˜ãActorãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆ
    model = Actor(
        lidar_dim=downsample_num,
        action_dim=action_dim,
        hidden_dim=hidden_dim,
        freeze_feature_layers=freeze_features  # å‡çµã‚ªãƒ—ã‚·ãƒ§ãƒ³
    )
    
    if 'actor_state_dict' in checkpoint:
        model.load_state_dict(checkpoint['actor_state_dict'])
        print("Loaded actor from 'actor_state_dict'")
    elif 'actor' in checkpoint:
        model.load_state_dict(checkpoint['actor'])
        print("Loaded actor from 'actor'")
    else:
        available_keys = list(checkpoint.keys())
        raise KeyError(f"No actor state found in checkpoint. Available keys: {available_keys}")
    
    # ãƒ­ãƒ¼ãƒ‰å¾Œã«å†åº¦å‡çµè¨­å®šã‚’é©ç”¨ï¼ˆstate_dictãƒ­ãƒ¼ãƒ‰å¾Œã«å¿…è¦ï¼‰
    if freeze_features:
        model.freeze_feature_layers()
        
    model.to(device)
    
    # å‡çµçŠ¶æ…‹ã‚’ç¢ºèª
    print("\n=== Actor Model Freeze Status ===")
    model.check_frozen_status()
    
    return model


def save_sac_checkpoint(original_checkpoint, model, optimizer, epoch, loss, save_path, freeze_info=None):
    """SACå½¢å¼ã§ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ä¿å­˜ï¼ˆå‡çµæƒ…å ±ä»˜ãï¼‰"""
    # å…ƒã®SACãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆæ§‹é€ ã‚’ã‚³ãƒ”ãƒ¼
    updated_checkpoint = {}
    
    # å…ƒã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã®å†…å®¹ã‚’ã‚³ãƒ”ãƒ¼ï¼ˆæ·±ã„ã‚³ãƒ”ãƒ¼ã§ã¯ãªãå‚ç…§ã‚³ãƒ”ãƒ¼ï¼‰
    for key, value in original_checkpoint.items():
        if key != 'actor_state_dict' and key != 'actor_optimizer':
            updated_checkpoint[key] = value
    
    # Actoré–¢é€£ã‚’æ›´æ–°
    updated_checkpoint['actor_state_dict'] = model.state_dict()
    updated_checkpoint['actor_optimizer'] = optimizer.state_dict()
    
    # ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æƒ…å ±ã‚’è¿½åŠ 
    updated_checkpoint['fine_tune_epoch'] = epoch
    updated_checkpoint['fine_tune_loss'] = loss
    updated_checkpoint['original_checkpoint_path'] = original_checkpoint.get('checkpoint_path', 'unknown')
    
    # ğŸ”¥ å‡çµæƒ…å ±ã‚’ä¿å­˜
    if freeze_info:
        updated_checkpoint['freeze_info'] = freeze_info
        updated_checkpoint['frozen_feature_layers'] = freeze_info.get('frozen', False)
    
    torch.save(updated_checkpoint, save_path)
    return updated_checkpoint


def create_optimizer_for_model(model, lr, freeze_mode=False):
    """ãƒ¢ãƒ‡ãƒ«ã®å‡çµçŠ¶æ…‹ã«å¿œã˜ã¦ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼ã‚’ä½œæˆ"""
    if freeze_mode:
        # å­¦ç¿’å¯èƒ½ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ã¿ã‚’å¯¾è±¡ã«ã™ã‚‹
        trainable_params = [p for p in model.parameters() if p.requires_grad]
        optimizer = torch.optim.Adam(trainable_params, lr=lr)
        
        total_params = sum(p.numel() for p in model.parameters())
        trainable_count = sum(p.numel() for p in trainable_params)
        
        print(f"å‡çµãƒ¢ãƒ¼ãƒ‰: å­¦ç¿’å¯èƒ½ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ {trainable_count:,}/{total_params:,} ({100*trainable_count/total_params:.1f}%)")
    else:
        # å…¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å¯¾è±¡ã«ã™ã‚‹
        optimizer = torch.optim.Adam(model.parameters(), lr=lr)
        total_params = sum(p.numel() for p in model.parameters())
        print(f"é€šå¸¸ãƒ¢ãƒ¼ãƒ‰: å…¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ {total_params:,} ãŒå­¦ç¿’å¯èƒ½")
    
    return optimizer


# è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ï¼ˆå®Ÿéš›ã®config.yamlãƒ•ã‚¡ã‚¤ãƒ«ã«è¿½åŠ æ¨å¥¨ï¼‰
DEFAULT_FREEZE_CONFIG = {
    'freeze_feature_layers': True,      # ç‰¹å¾´æŠ½å‡ºå±¤ã‚’å‡çµã™ã‚‹ã‹
    'unfreeze_at_epoch': None,          # æŒ‡å®šã‚¨ãƒãƒƒã‚¯ã§å‡çµè§£é™¤ï¼ˆNoneã§ç„¡åŠ¹ï¼‰
    'freeze_schedule': [],              # ã‚¨ãƒãƒƒã‚¯ã¨å‡çµçŠ¶æ…‹ã®ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«
    'freeze_lr_multiplier': 0.1,        # å‡çµæ™‚ã®å­¦ç¿’ç‡å€ç‡
}


@hydra.main(config_path="config", config_name="train_cnn", version_base="1.2")
def main(cfg: DictConfig) -> None:
    print("=== SAC Fine-tuning with Freezing Configuration ===")
    print(OmegaConf.to_yaml(cfg))
    
    # ğŸ”¥ å‡çµè¨­å®šã®ãƒãƒ¼ã‚¸ï¼ˆè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã«ç„¡ã„å ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆä½¿ç”¨ï¼‰
    freeze_cfg = OmegaConf.merge(DEFAULT_FREEZE_CONFIG, cfg.get('freeze', {}))
    print(f"\n=== Freeze Configuration ===")
    print(OmegaConf.to_yaml(freeze_cfg))
    print("=" * 50)

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"Using device: {device}")
    
    data_path = hydra.utils.to_absolute_path(cfg.data_path)
    print(f"Data path: {data_path}")
    
    # --- ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¨ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã®æº–å‚™ ---
    print("\n--- Preparing Data Loaders ---")
    
    # æ³¨æ„: å®Ÿéš›ã®ç’°å¢ƒã§ã¯ä»¥ä¸‹ã®importãŒå¿…è¦
    # from src.data.dataset.dataset import HybridLoader 
    # from src.data.dataset.transform import SeqToSeqTransform, StreamAugmentor
    
    # ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã®è¨­å®šï¼ˆå…ƒã‚³ãƒ¼ãƒ‰ã¨åŒã˜ï¼‰
    # ã“ã“ã§ã¯ç°¡ç•¥åŒ–ã®ãŸã‚ã€å®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼åˆæœŸåŒ–ã¯çœç•¥
    print("ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼åˆæœŸåŒ–ã¯å®Ÿè£…ç’°å¢ƒã«å¿œã˜ã¦è¡Œã£ã¦ãã ã•ã„")
    
    # ä»®æƒ³çš„ãªãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ï¼ˆãƒ†ã‚¹ãƒˆç”¨ï¼‰
    class DummyDataLoader:
        def __init__(self, batch_size, sequence_length, input_dim):
            self.batch_size = batch_size
            self.sequence_length = sequence_length
            self.input_dim = input_dim
            self.batches = 10  # ä»®æƒ³ãƒãƒƒãƒæ•°
            
        def __iter__(self):
            for _ in range(self.batches):
                yield {
                    'scan_seq': torch.randn(self.batch_size, self.sequence_length, self.input_dim),
                    'target_action_seq': torch.randn(self.batch_size, self.sequence_length, 2)
                }
                
        def __len__(self):
            return self.batches
    
    # ä»®æƒ³ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ï¼ˆå®Ÿéš›ã®ç’°å¢ƒã§ã¯ä¸Šè¨˜ã®HybridLoaderã‚’ä½¿ç”¨ï¼‰
    train_loader = DummyDataLoader(
        batch_size=cfg.batch_size,
        sequence_length=cfg.sequence_length,
        input_dim=cfg.input_dim
    )
    
    print(f"Data loader prepared with batch size: {cfg.batch_size}")
    
    # --- SAC ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ãƒ­ãƒ¼ãƒ‰ ---
    print("\n--- Loading SAC Checkpoint ---")
    ckpt_path = "/home/ktr/rl_ws/ckpts/sac/TAL/2025-06-16/19-13-34/best_model_reward_291.18.pth"
    print(f"Checkpoint path: {ckpt_path}")
    
    try:
        # å…ƒã®SACãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆå…¨ä½“ã‚’ãƒ­ãƒ¼ãƒ‰
        original_checkpoint = load_sac_checkpoint(ckpt_path, device)
        
        # ğŸ”¥ å‡çµæ©Ÿèƒ½ä»˜ãActorãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆ
        downsample_num = 100
        action_dim = 2
        hidden_dim = 256
        
        model = create_actor_from_checkpoint(
            checkpoint=original_checkpoint, 
            downsample_num=downsample_num, 
            action_dim=action_dim, 
            hidden_dim=hidden_dim, 
            device=device,
            freeze_features=freeze_cfg.freeze_feature_layers  # ğŸ”¥ å‡çµè¨­å®š
        )
        
        print(f"Successfully loaded Actor model from SAC checkpoint")
        print(f"Model parameters: {sum(p.numel() for p in model.parameters()):,}")
        
    except Exception as e:
        print(f"Error loading SAC checkpoint: {e}")
        # ä»£æ›¿ã¨ã—ã¦æ–°ã—ã„ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆ
        print("Creating new model instead...")
        model = Actor(
            lidar_dim=100,
            action_dim=2,
            hidden_dim=256,
            freeze_feature_layers=freeze_cfg.freeze_feature_layers
        ).to(device)
        original_checkpoint = {}
    
    # --- æå¤±é–¢æ•°ã¨æœ€é©åŒ–æ‰‹æ³•ã®æº–å‚™ ---
    criterion = torch.nn.SmoothL1Loss()
    
    # ğŸ”¥ å‡çµçŠ¶æ…‹ã«å¿œã˜ãŸå­¦ç¿’ç‡èª¿æ•´
    lr = cfg.lr
    if freeze_cfg.freeze_feature_layers:
        lr *= freeze_cfg.get('freeze_lr_multiplier', 1.0)
        print(f"å‡çµãƒ¢ãƒ¼ãƒ‰ã«ã‚ˆã‚Šå­¦ç¿’ç‡ã‚’èª¿æ•´: {cfg.lr} â†’ {lr}")
    
    # ğŸ”¥ å‡çµçŠ¶æ…‹ã«å¿œã˜ãŸã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼ä½œæˆ
    optimizer = create_optimizer_for_model(model, lr, freeze_cfg.freeze_feature_layers)
    
    print(f"Using SmoothL1Loss with Adam optimizer (lr={lr})")

    # --- ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã¨æ—©æœŸçµ‚äº†ã®æº–å‚™ ---
    save_path = cfg.ckpt_path
    os.makedirs(save_path, exist_ok=True)
    
    # æ—¥ä»˜æ™‚é–“ãƒ•ã‚©ãƒ«ãƒ€ã‚’ä½œæˆ
    timestamp = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
    freeze_suffix = "_frozen" if freeze_cfg.freeze_feature_layers else "_full"
    datetime_save_path = os.path.join(save_path, f"fine_tuning{freeze_suffix}_{timestamp}")
    os.makedirs(datetime_save_path, exist_ok=True)
    
    early_stop_epochs = cfg.early_stop_epochs
    patience_counter = 0
    top_k = 3
    top_k_checkpoints = []
    
    print(f"Checkpoints will be saved to: {datetime_save_path}")
    print(f"Early stopping patience: {early_stop_epochs}")

    # -------------------
    # ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ«ãƒ¼ãƒ—ï¼ˆå‡çµæ©Ÿèƒ½ä»˜ãï¼‰
    # -------------------
    print(f"\n=== Start Fine-tuning: {cfg.model_name} ===")
    print(f"å‡çµè¨­å®š: {'æœ‰åŠ¹' if freeze_cfg.freeze_feature_layers else 'ç„¡åŠ¹'}")
    
    for epoch in range(cfg.num_epochs):
        # ğŸ”¥ ã‚¨ãƒãƒƒã‚¯ãƒ™ãƒ¼ã‚¹ã®å‡çµã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«å‡¦ç†
        if freeze_cfg.get('unfreeze_at_epoch') == epoch:
            print(f"\nğŸ”¥ ã‚¨ãƒãƒƒã‚¯ {epoch}: å‡çµè§£é™¤")
            model.unfreeze_feature_layers()
            # ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼ã‚’å†æ§‹ç¯‰
            optimizer = create_optimizer_for_model(model, cfg.lr, freeze_mode=False)
            
        # ğŸ”¥ è¤‡æ•°ã‚¹ãƒ†ãƒƒãƒ—ã®å‡çµã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«å‡¦ç†
        for schedule_item in freeze_cfg.get('freeze_schedule', []):
            if schedule_item['epoch'] == epoch:
                if schedule_item['action'] == 'unfreeze':
                    print(f"\nğŸ”¥ ã‚¨ãƒãƒƒã‚¯ {epoch}: ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«å‡çµè§£é™¤")
                    model.unfreeze_feature_layers()
                    optimizer = create_optimizer_for_model(model, cfg.lr, freeze_mode=False)
                elif schedule_item['action'] == 'freeze':
                    print(f"\nğŸ”¥ ã‚¨ãƒãƒƒã‚¯ {epoch}: ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«å‡çµ")
                    model.freeze_feature_layers()
                    lr_frozen = cfg.lr * freeze_cfg.get('freeze_lr_multiplier', 1.0)
                    optimizer = create_optimizer_for_model(model, lr_frozen, freeze_mode=True)
        
        model.train()
        running_loss = 0.0
        batch_count = 0

        for batch in train_loader:
            # --- ãƒãƒƒãƒãƒ‡ãƒ¼ã‚¿ã®å–å¾— ---
            scan_seq = batch['scan_seq'].to(device)  # 2D LiDARã®ã‚¹ã‚­ãƒ£ãƒ³
            target_seq = batch['target_action_seq'].to(device)
            
            # --- ãƒ¢ãƒ‡ãƒ«ã«å…¥åŠ›ã™ã‚‹ ---        
            _, _, action = model.sample(scan_seq)

            # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã®å½¢çŠ¶ã‚’èª¿æ•´
            if action.dim() == 2:
                target = target_seq[:, -1, :]  # æœ€å¾Œã®ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³
            else:
                raise ValueError(f"Unsupported output shape: {action.shape}")

            # æå¤±è¨ˆç®—ã¨ãƒãƒƒã‚¯ãƒ—ãƒ­ãƒ‘ã‚²ãƒ¼ã‚·ãƒ§ãƒ³
            loss = criterion(action, target)
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            running_loss += loss.item()
            batch_count += 1

        # ã‚¨ãƒãƒƒã‚¯çµ‚äº†æ™‚ã®å‡¦ç†
        avg_loss = running_loss / len(train_loader)
        
        # å‡çµçŠ¶æ…‹ã®è¡¨ç¤º
        frozen_params = sum(1 for p in model.parameters() if not p.requires_grad)
        total_params = sum(1 for p in model.parameters())
        freeze_status = f"å‡çµ: {frozen_params}/{total_params}" if frozen_params > 0 else "å…¨ã¦å­¦ç¿’å¯èƒ½"
        
        print(f"[Epoch {epoch+1:3d}/{cfg.num_epochs}] Loss: {avg_loss:.6f} | {freeze_status} | Batches: {batch_count}")

        # Top-k ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜ï¼ˆSACå½¢å¼ã§ä¿å­˜ã€å‡çµæƒ…å ±ä»˜ãï¼‰
        if len(top_k_checkpoints) < top_k or avg_loss < top_k_checkpoints[-1][0]:
            checkpoint_filename = f'finetuned_epoch_{epoch+1:03d}_loss_{avg_loss:.6f}.pth'
            checkpoint_path = os.path.join(datetime_save_path, checkpoint_filename)
            
            try:
                # ğŸ”¥ å‡çµæƒ…å ±ã‚’å«ã‚ã‚‹
                freeze_info = {
                    'frozen': any(not p.requires_grad for p in model.parameters()),
                    'freeze_status': model.check_frozen_status(),
                    'epoch': epoch + 1,
                    'config': freeze_cfg
                }
                
                # SACå½¢å¼ã§ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ä¿å­˜
                updated_checkpoint = save_sac_checkpoint(
                    original_checkpoint=original_checkpoint,
                    model=model,
                    optimizer=optimizer,
                    epoch=epoch + 1,
                    loss=avg_loss,
                    save_path=checkpoint_path,
                    freeze_info=freeze_info  # ğŸ”¥ å‡çµæƒ…å ±è¿½åŠ 
                )
                
                print(f"  [âœ”] Saved fine-tuned SAC model: {checkpoint_filename}")
                print(f"      ğŸ“ Location: {datetime_save_path}")
                
                # Top-kç®¡ç†
                top_k_checkpoints.append((avg_loss, epoch + 1, checkpoint_path))
                top_k_checkpoints.sort(key=lambda x: x[0])  # æå¤±ã§ã‚½ãƒ¼ãƒˆ
                
                # å¤ã„ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’å‰Šé™¤
                if len(top_k_checkpoints) > top_k:
                    worst_checkpoint = top_k_checkpoints.pop()
                    if os.path.exists(worst_checkpoint[2]):
                        os.remove(worst_checkpoint[2])
                        print(f"  [ğŸ—‘] Removed old model: {os.path.basename(worst_checkpoint[2])}")
                
                patience_counter = 0  # æ”¹å–„ãŒã‚ã£ãŸã®ã§ã‚«ã‚¦ãƒ³ã‚¿ãƒ¼ãƒªã‚»ãƒƒãƒˆ
                
            except Exception as e:
                print(f"  [âŒ] Error saving checkpoint: {e}")
                
        else:
            patience_counter += 1
            best_loss = top_k_checkpoints[0][0] if top_k_checkpoints else float('inf')
            print(f"  [!] No improvement over best loss ({best_loss:.6f}). Patience: {patience_counter}/{early_stop_epochs}")
            
            # æ—©æœŸçµ‚äº†ãƒã‚§ãƒƒã‚¯
            if patience_counter >= early_stop_epochs:
                print(f"[â¹] Early stopping triggered after {epoch+1} epochs.")
                break

    print(f"\n=== Fine-tuning Completed ===")
    print("Top fine-tuned models saved:")
    for i, (loss_val, epoch_num, path) in enumerate(top_k_checkpoints, 1):
        print(f"  {i}. Epoch: {epoch_num:3d}, Loss: {loss_val:.6f}")
        print(f"     Path: {path}")
    
    # æœ€çµ‚çµ±è¨ˆ
    if top_k_checkpoints:
        best_loss = top_k_checkpoints[0][0]
        best_epoch = top_k_checkpoints[0][1]
        print(f"\nBest model: Epoch {best_epoch}, Loss: {best_loss:.6f}")
    
    # ğŸ”¥ æœ€çµ‚çš„ãªå‡çµçŠ¶æ…‹ã‚’è¡¨ç¤º
    print(f"\n=== Final Model Status ===")
    model.check_frozen_status()
    
    print("Fine-tuning finished successfully!")


if __name__ == "__main__":
    main()