import torch
from torch.utils.data import DataLoader
import os
import hydra
from omegaconf import DictConfig, OmegaConf

# å¤–éƒ¨ãƒ•ã‚¡ã‚¤ãƒ«ã®importã‚’æ–°ã—ã„ã‚‚ã®ã«æ›´æ–°
from src.models.models import load_cnn_model # ã”è‡ªèº«ã®ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿é–¢æ•°ã«ç½®ãæ›ãˆã¦ãã ã•ã„
from src.data.dataset.lidar_dataset import LidarSeqToSeqDataset
from src.data.dataset.transform import SeqToSeqTransform

@hydra.main(config_path="config", config_name="train_cnn", version_base="1.2")
def main(cfg: DictConfig) -> None:
    """
    Hydraã«ã‚ˆã£ã¦è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã€ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ã‚’è¡Œã†ãƒ¡ã‚¤ãƒ³é–¢æ•°ã€‚
    prev_actionã®ä½¿ç”¨æœ‰ç„¡ã‚„ã€ãƒ¢ãƒ‡ãƒ«ã®å‡ºåŠ›å½¢å¼ã«æŸ”è»Ÿã«å¯¾å¿œã™ã‚‹ã€‚
    """
    print("--- Configuration ---")
    print(OmegaConf.to_yaml(cfg))
    print("---------------------")

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # --- ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¨ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã®æº–å‚™ ---
    # æ–°ã—ã„Transformã¨Datasetã‚’ä½¿ç”¨
    transform = SeqToSeqTransform(
        range_max=cfg.range_max, 
        base_num=1081, # Lidarã®å…ƒã®ç‚¹ç¾¤æ•°ã«åˆã‚ã›ã¦èª¿æ•´
        downsample_num=cfg.input_dim
    )
    # to_absolute_pathã§çµ¶å¯¾ãƒ‘ã‚¹ã«å¤‰æ›
    data_path = hydra.utils.to_absolute_path(cfg.data_path)
    train_dataset = LidarSeqToSeqDataset(
        root_dir=data_path, 
        sequence_length=cfg.sequence_length, 
        transform=transform
    )
    train_loader = DataLoader(train_dataset, batch_size=cfg.batch_size, shuffle=True)

    # --- ãƒ¢ãƒ‡ãƒ«ã€æå¤±é–¢æ•°ã€æœ€é©åŒ–æ‰‹æ³•ã®æº–å‚™ ---
    # ãƒ¢ãƒ‡ãƒ«åã¯è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰å–å¾—
    model = load_cnn_model(
        model_name=cfg.model_name, 
        input_dim=cfg.input_dim, 
        output_dim=cfg.output_dim
    ).to(device)
    criterion = torch.nn.SmoothL1Loss()
    optimizer = torch.optim.Adam(model.parameters(), lr=cfg.lr)

    # --- ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã¨æ—©æœŸçµ‚äº†ã®æº–å‚™ (å¤‰æ›´ãªã—) ---
    save_path = cfg.ckpt_path
    os.makedirs(save_path, exist_ok=True)

    early_stop_epochs = cfg.early_stop_epochs
    patience_counter = 0
    
    top_k = 3
    top_k_checkpoints = [] 

    # -------------------
    # å­¦ç¿’ãƒ«ãƒ¼ãƒ—
    # -------------------
    print(f"\n--- Start Training: {cfg.model_name} ---")
    for epoch in range(cfg.num_epochs):
        model.train()
        running_loss = 0.0

        for batch in train_loader:
            # å¿…è¦ãªãƒ‡ãƒ¼ã‚¿ã‚’ã™ã¹ã¦ãƒ‡ãƒã‚¤ã‚¹ã«é€ã‚‹
            scan_seq = batch['scan_seq'].to(device)
            prev_action_seq = batch['prev_action_seq'].to(device)
            target_seq = batch['target_action_seq'].to(device)

            # --- ãƒ¢ãƒ‡ãƒ«ã¸ã®å…¥åŠ›ã‚’å‹•çš„ã«åˆ‡ã‚Šæ›¿ãˆ ---
            if cfg.model_use_prev_action:
                # prev_action ã‚’ä½¿ã†ãƒ¢ãƒ‡ãƒ«ã®å ´åˆ
                output = model(scan_seq, prev_action_seq)
            else:
                # prev_action ã‚’ä½¿ã‚ãªã„ãƒ¢ãƒ‡ãƒ«ã®å ´åˆ (LiDARã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã®ã¿å…¥åŠ›)
                output = model(scan_seq)

            # --- ãƒ¢ãƒ‡ãƒ«ã®å‡ºåŠ›å½¢çŠ¶ã«åˆã‚ã›ã¦æ•™å¸«ãƒ‡ãƒ¼ã‚¿ã®å½¢çŠ¶ã‚’æ•´å½¢ ---
            if output.dim() == 3 and output.shape[1] > 1:
                # å‡ºåŠ›ãŒæ™‚ç³»åˆ— [B, SeqLen, F] ã®å ´åˆ -> æ•™å¸«ãƒ‡ãƒ¼ã‚¿ã‚‚æ™‚ç³»åˆ—ã«
                target = target_seq
            elif output.dim() == 2:
                # å‡ºåŠ›ãŒéæ™‚ç³»åˆ— [B, F] ã®å ´åˆ -> æ•™å¸«ãƒ‡ãƒ¼ã‚¿ã¯ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã®æœ€å¾Œã®ãƒ•ãƒ¬ãƒ¼ãƒ 
                target = target_seq[:, -1, :]
            else:
                # ãã®ä»–ã®æƒ³å®šå¤–ã®å½¢çŠ¶
                raise ValueError(f"Unsupported output shape: {output.shape}")

            # æå¤±è¨ˆç®—ã¨é€†ä¼æ’­
            loss = criterion(output, target)
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            running_loss += loss.item()

        avg_loss = running_loss / len(train_loader)
        print(f"[Epoch {epoch+1}/{cfg.num_epochs}] Loss: {avg_loss:.4f}")

        if len(top_k_checkpoints) < top_k or avg_loss < top_k_checkpoints[-1][0]:
            checkpoint_path = os.path.join(save_path, f'model_epoch_{epoch+1}_loss_{avg_loss:.4f}.pth')
            torch.save(model.state_dict(), checkpoint_path)
            print(f"  [âœ”] Saved new top-k model (loss={avg_loss:.4f})")
            top_k_checkpoints.append((avg_loss, epoch + 1, checkpoint_path))
            top_k_checkpoints.sort(key=lambda x: x[0])
            if len(top_k_checkpoints) > top_k:
                worst_checkpoint = top_k_checkpoints.pop()
                if os.path.exists(worst_checkpoint[2]):
                    os.remove(worst_checkpoint[2])
                    print(f"  [ğŸ—‘] Removed old model: {os.path.basename(worst_checkpoint[2])}")
            patience_counter = 0
        else:
            patience_counter += 1
            best_loss = top_k_checkpoints[0][0] if top_k_checkpoints else float('inf')
            print(f"  [!] No improvement over best loss ({best_loss:.4f}). Patience: {patience_counter}/{early_stop_epochs}")
            if patience_counter >= early_stop_epochs:
                print("[â¹] Early stopping triggered.")
                break

    print("\n--- Training Finished ---")
    print("Top models saved:")
    for loss_val, epoch_num, path in top_k_checkpoints:
        print(f"  - Epoch: {epoch_num}, Loss: {loss_val:.4f}, Path: {path}")


if __name__ == "__main__":
    main()